{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import random\n",
    "# import tempfile\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import *\n",
    "\n",
    "from boruta import boruta_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percentage of nulls on pandas dataframe\n",
    "def val_pd_df_nan(df):\n",
    "    flat_data = df.values.flatten()\n",
    "    count=0\n",
    "    for value in flat_data:\n",
    "        if value is not None:\n",
    "            continue\n",
    "        count+= 1\n",
    "    return round(100*count/len(flat_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv('data/aps_failure_training_set_processed_8bit.csv', na_values='na')\n",
    "test_ds =  pd.read_csv('data/aps_failure_test_set_processed_8bit.csv', na_values='na')\n",
    "\n",
    "train_labels = train_ds['class']\n",
    "test_labels = test_ds['class']\n",
    "train_features = train_ds.drop('class', axis=1)\n",
    "test_features = test_ds.drop('class', axis=1)\n",
    "\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.apply(round)\n",
    "train_labels = train_labels.replace({-1:0})\n",
    "\n",
    "test_labels = test_labels.apply(round)\n",
    "test_labels = test_labels.replace({-1:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = 1000\n",
    "\n",
    "idxs_pos = train_labels[train_labels==1].index\n",
    "idxs_neg = train_labels[train_labels==0].sample(n=number_samples, replace=False, random_state=0).index\n",
    "idxs_balanced = np.concatenate((idxs_pos,idxs_neg))\n",
    "train_features_balanced = train_features.loc[idxs_balanced]\n",
    "train_labels_balanced = train_labels.loc[idxs_balanced]\n",
    "print(f'Proportion balanced: {int(number_samples/1000)}/1')\n",
    "print(train_labels_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_samples = 375\n",
    "\n",
    "# idxs_pos = test_labels[test_labels==1].index\n",
    "# idxs_neg = test_labels[test_labels==0].sample(n=number_samples, replace=False, random_state=0).index\n",
    "# idxs_balanced = np.concatenate((idxs_pos,idxs_neg))\n",
    "# test_features_balanced = test_features.loc[idxs_balanced]\n",
    "# test_labels_balanced = test_labels.loc[idxs_balanced]\n",
    "# print(f'Proportion balanced: {int(number_samples/1000)}/1')\n",
    "# print(test_labels_balanced.value_counts())\n",
    "\n",
    "test_features_balanced = test_features\n",
    "test_labels_balanced = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_features_balanced)\n",
    "\n",
    "# # model export\n",
    "# joblib.dump(scaler, \"models/MinMaxScaler.save\") \n",
    "# scaler = joblib.load(\"models/MinMaxScaler.save\") \n",
    "\n",
    "train_features_balanced = pd.DataFrame(scaler.transform(train_features_balanced), columns=train_features_balanced.columns)\n",
    "test_features_balanced = pd.DataFrame(scaler.transform(test_features_balanced), columns=test_features_balanced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://host.docker.internal:5000\")\n",
    "mlflow.set_experiment(\"demo_ml\")\n",
    "print(\"Running {} with tracking URI {}\".format(sys.argv[0], mlflow.get_tracking_uri()))\n",
    "\n",
    "def ml_tracking(params, metrics, models):\n",
    "    with mlflow.start_run():\n",
    "        # log params\n",
    "        for param, value in params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "        # log metrics\n",
    "        for metric, value in metrics.items():\n",
    "            mlflow.log_metric(metric, value)\n",
    "        # log models\n",
    "        for key, model in models.items():\n",
    "            mlflow.sklearn.log_model(model, key)\n",
    "#         run_id = mlflow.active_run().info.run_id\n",
    "#         service = mlflow.tracking.MlflowClient()\n",
    "#         run = service.get_run(run_id)\n",
    "#         print(\"Metadata & data for run with UUID %s: %s\" % (run_id, run))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectKBest = SelectKBest(chi2, 88)\n",
    "pca = PCA(0.95)\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "borutaSelector = boruta_py.BorutaPy(rf, n_estimators='auto', verbose=0, random_state=123)\n",
    "\n",
    "# training and validation template\n",
    "def run(pipeline, train_X, train_y, test_X, test_y, fit_predict_arr=False):\n",
    "    \n",
    "    # train\n",
    "    pipeline.fit(train_X.values, train_y.values) if fit_predict_arr else pipeline.fit(train_X, train_y)\n",
    "    \n",
    "    # predict train\n",
    "    y_pred = pipeline.predict(train_X.values) if fit_predict_arr else pipeline.predict(train_X)\n",
    "    train_report = classification_report(train_y, y_pred)\n",
    "    # print(train_report)\n",
    "    \n",
    "    # predict test\n",
    "    y_pred = pipeline.predict(test_X.values) if fit_predict_arr else pipeline.predict(test_X)\n",
    "    test_report = classification_report(test_y, y_pred)\n",
    "    print(test_report)\n",
    "    \n",
    "    # generate confusion matrix\n",
    "    cm = confusion_matrix(test_y, y_pred).ravel()\n",
    "    cm = pd.DataFrame(cm.reshape((1,4)), columns=['tn', 'fp', 'fn', 'tp'])\n",
    "    print(cm)\n",
    "    \n",
    "    # generate cost\n",
    "    total_cost = 10 * cm.fp + 500 * cm.fn\n",
    "    print(f'Total cost is:{float(total_cost.values[0])}')\n",
    "    \n",
    "    # mlflow tracking\n",
    "    ml_tracking(\n",
    "        params={}, \n",
    "        metrics={\n",
    "            \"cost\": float(total_cost),\n",
    "            \"weighted_f1\": f1_score(test_y, y_pred, average='weighted'),\n",
    "            \"accuracy_score\": accuracy_score(test_y, y_pred)\n",
    "        }, \n",
    "        models={\n",
    "            \"MinMaxScaler\": scaler,\n",
    "            \"Pipeline\": pipeline\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KBest + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbest + rf pipeline\n",
    "kbest_pipeline = Pipeline([('selectKBest', selectKBest), ('rf', rf)])\n",
    "\n",
    "# execute training and validation\n",
    "run(kbest_pipeline, train_features_balanced, train_labels_balanced, test_features_balanced, test_labels_balanced)\n",
    "\n",
    "print(\"features count:\", len(train_features_balanced.columns))\n",
    "print(\"features count after kbest:\", len(train_features_balanced.columns[kbest_pipeline[0].get_support()]))\n",
    "print(\"features selected:\", train_features_balanced.columns[kbest_pipeline[0].get_support()].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca + rf pipeline\n",
    "pca_pipeline = Pipeline([('pca', pca), ('rf', rf)])\n",
    "\n",
    "# execute training and validation\n",
    "run(pca_pipeline, train_features_balanced, train_labels_balanced, test_features_balanced, test_labels_balanced)\n",
    "\n",
    "print(\"features count:\", len(train_features_balanced.columns))\n",
    "print(\"pca components:\", pca_pipeline[0].n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta + RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boruta + rf pipeline\n",
    "boruta_pipeline = Pipeline([('borutaSelector', borutaSelector), ('rf', rf)])\n",
    "\n",
    "# execute training and validation\n",
    "run(boruta_pipeline, train_features_balanced, train_labels_balanced, test_features_balanced, test_labels_balanced, True)\n",
    "\n",
    "print(\"features count:\", len(train_features_balanced.columns))\n",
    "print(\"features count after boruta:\", len(train_features_balanced.columns[boruta_pipeline[0].support_]))\n",
    "print(\"features selected:\", train_features_balanced.columns[boruta_pipeline[0].support_].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rf pipeline\n",
    "rf_pipeline = Pipeline([('rf', rf)])\n",
    "\n",
    "# execute training and validation\n",
    "run(rf_pipeline, train_features_balanced, train_labels_balanced, test_features_balanced, test_labels_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
